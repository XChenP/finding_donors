
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{finding\_donors}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Machine Learning Engineer
Nanodegree}\label{machine-learning-engineer-nanodegree}

\subsection{Supervised Learning}\label{supervised-learning}

\subsection{\texorpdfstring{Project: Finding Donors for
\emph{CharityML}}{Project: Finding Donors for CharityML}}\label{project-finding-donors-for-charityml}

    \subsection{Getting Started}\label{getting-started}

This project on modeling individuals' income used data collected from
the 1994 U.S. Census. The goal with this implementation is to construct
a model that accurately predicts whether an individual makes more than
\$50,000. This sort of task can arise in a non-profit setting, where
organizations survive on donations. Understanding an individual's income
can help a non-profit better understand how large of a donation to
request, or whether or not they should reach out to begin with. While it
can be difficult to determine an individual's general income bracket
directly from public sources, we can infer this value from other
publically available features.

The dataset for this project originates from the
\href{https://archive.ics.uci.edu/ml/datasets/Census+Income}{UCI Machine
Learning Repository}. The datset was donated by Ron Kohavi and Barry
Becker, after being published in the article \emph{"Scaling Up the
Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"}. The
article by Ron Kohavi
\href{https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf}{online}. The
data investigated here consists of small changes to the original
dataset, such as removing the
\texttt{\textquotesingle{}fnlwgt\textquotesingle{}} feature and records
with missing or ill-formatted entries.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Exploring the Data}\label{exploring-the-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Import libraries necessary for this project}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{from} \PY{n+nn}{time} \PY{k}{import} \PY{n}{time}
        \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{display} \PY{c+c1}{\PYZsh{} Allows the use of display() for DataFrames}
        
        \PY{c+c1}{\PYZsh{} Import supplementary visualization code visuals.py}
        \PY{k+kn}{import} \PY{n+nn}{visuals} \PY{k}{as} \PY{n+nn}{vs}
        
        \PY{c+c1}{\PYZsh{} Pretty display for notebooks}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{c+c1}{\PYZsh{} Load the Census dataset}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{census.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Success \PYZhy{} Display the first record}
        \PY{n}{display}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{n}{n}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   age   workclass education_level  education-num  marital-status  \
0   39   State-gov       Bachelors           13.0   Never-married   

      occupation    relationship    race    sex  capital-gain  capital-loss  \
0   Adm-clerical   Not-in-family   White   Male        2174.0           0.0   

   hours-per-week  native-country income  
0            40.0   United-States  <=50K  
    \end{verbatim}

    
    \subsubsection{Implementation: Data
Exploration}\label{implementation-data-exploration}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Total number of records}
        \PY{n}{n\PYZus{}records} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Number of records where individual\PYZsq{}s income is more than \PYZdl{}50,000}
        \PY{n}{n\PYZus{}greater\PYZus{}50k} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{income}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZgt{}50K}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Number of records where individual\PYZsq{}s income is at most \PYZdl{}50,000}
        \PY{n}{n\PYZus{}at\PYZus{}most\PYZus{}50k} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{income}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}=50K}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Percentage of individuals whose income is more than \PYZdl{}50,000}
        \PY{n}{greater\PYZus{}percent} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{n\PYZus{}greater\PYZus{}50k} \PY{o}{/} \PY{p}{(}\PY{n}{n\PYZus{}greater\PYZus{}50k} \PY{o}{+} \PY{n}{n\PYZus{}at\PYZus{}most\PYZus{}50k}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Print the results}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Total number of records: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}records}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Individuals making more than \PYZdl{}50,000: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}greater\PYZus{}50k}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Individuals making at most \PYZdl{}50,000: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}at\PYZus{}most\PYZus{}50k}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Percentage of individuals making more than \PYZdl{}50,000: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{greater\PYZus{}percent}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Total number of records: 45222
Individuals making more than \$50,000: 11208
Individuals making at most \$50,000: 34014
Percentage of individuals making more than \$50,000: 24.78\%

    \end{Verbatim}

    ** Featureset Exploration **

\begin{itemize}
\tightlist
\item
  \textbf{age}: continuous.
\item
  \textbf{workclass}: Private, Self-emp-not-inc, Self-emp-inc,
  Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
\item
  \textbf{education}: Bachelors, Some-college, 11th, HS-grad,
  Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters,
  1st-4th, 10th, Doctorate, 5th-6th, Preschool.
\item
  \textbf{education-num}: continuous.
\item
  \textbf{marital-status}: Married-civ-spouse, Divorced, Never-married,
  Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
\item
  \textbf{occupation}: Tech-support, Craft-repair, Other-service, Sales,
  Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,
  Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv,
  Protective-serv, Armed-Forces.
\item
  \textbf{relationship}: Wife, Own-child, Husband, Not-in-family,
  Other-relative, Unmarried.
\item
  \textbf{race}: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo,
  Other.
\item
  \textbf{sex}: Female, Male.
\item
  \textbf{capital-gain}: continuous.
\item
  \textbf{capital-loss}: continuous.
\item
  \textbf{hours-per-week}: continuous.
\item
  \textbf{native-country}: United-States, Cambodia, England,
  Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India,
  Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy,
  Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France,
  Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary,
  Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador,
  Trinadad\&Tobago, Peru, Hong, Holand-Netherlands.
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Preparing the Data}\label{preparing-the-data}

    \subsubsection{Transforming Skewed Continuous
Features}\label{transforming-skewed-continuous-features}

A dataset may sometimes contain at least one feature whose values tend
to lie near a single number, but will also have a non-trivial number of
vastly larger or smaller values than that single number. Algorithms can
be sensitive to such distributions of values and can underperform if the
range is not properly normalized. With the census dataset two features
fit this description: '\texttt{capital-gain\textquotesingle{}} and
\texttt{\textquotesingle{}capital-loss\textquotesingle{}}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Split the data into features and target label}
        \PY{n}{income\PYZus{}raw} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{income}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{features\PYZus{}raw} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{income}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Visualize skewed continuous features of original data}
        \PY{n}{vs}\PY{o}{.}\PY{n}{distribution}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For highly-skewed feature distributions such as
\texttt{\textquotesingle{}capital-gain\textquotesingle{}} and
\texttt{\textquotesingle{}capital-loss\textquotesingle{}}, it is common
practice to apply a logarithmic transformation on the data so that the
very large and very small values do not negatively affect the
performance of a learning algorithm. Using a logarithmic transformation
significantly reduces the range of values caused by outliers. Care must
be taken when applying this transformation however: The logarithm of
\texttt{0} is undefined, so we must translate the values by a small
amount above \texttt{0} to apply the the logarithm successfully.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Log\PYZhy{}transform the skewed features}
        \PY{n}{skewed} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{capital\PYZhy{}gain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{capital\PYZhy{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{features\PYZus{}log\PYZus{}transformed} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{features\PYZus{}raw}\PY{p}{)}
        \PY{n}{features\PYZus{}log\PYZus{}transformed}\PY{p}{[}\PY{n}{skewed}\PY{p}{]} \PY{o}{=} \PY{n}{features\PYZus{}raw}\PY{p}{[}\PY{n}{skewed}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{x} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Visualize the new log distributions}
        \PY{n}{vs}\PY{o}{.}\PY{n}{distribution}\PY{p}{(}\PY{n}{features\PYZus{}log\PYZus{}transformed}\PY{p}{,} \PY{n}{transformed} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Normalizing Numerical
Features}\label{normalizing-numerical-features}

In addition to performing transformations on features that are highly
skewed, it is often good practice to perform some type of scaling on
numerical features. Applying a scaling to the data does not change the
shape of each feature's distribution (such as
\texttt{\textquotesingle{}capital-gain\textquotesingle{}} or
\texttt{\textquotesingle{}capital-loss\textquotesingle{}} above);
however, normalization ensures that each feature is treated equally when
applying supervised learners.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Import sklearn.preprocessing.StandardScaler}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{MinMaxScaler}
        
        \PY{c+c1}{\PYZsh{} Initialize a scaler, then apply it to the features}
        \PY{n}{scaler} \PY{o}{=} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} default=(0, 1)}
        \PY{n}{numerical} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{education\PYZhy{}num}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{capital\PYZhy{}gain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{capital\PYZhy{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hours\PYZhy{}per\PYZhy{}week}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
        \PY{n}{features\PYZus{}log\PYZus{}minmax\PYZus{}transform} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{features\PYZus{}log\PYZus{}transformed}\PY{p}{)}
        \PY{n}{features\PYZus{}log\PYZus{}minmax\PYZus{}transform}\PY{p}{[}\PY{n}{numerical}\PY{p}{]} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{features\PYZus{}log\PYZus{}transformed}\PY{p}{[}\PY{n}{numerical}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Show an example of a record with scaling applied}
        \PY{n}{display}\PY{p}{(}\PY{n}{features\PYZus{}log\PYZus{}minmax\PYZus{}transform}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{n}{n} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
        age          workclass education_level  education-num  \
0  0.301370          State-gov       Bachelors       0.800000   
1  0.452055   Self-emp-not-inc       Bachelors       0.800000   
2  0.287671            Private         HS-grad       0.533333   
3  0.493151            Private            11th       0.400000   
4  0.150685            Private       Bachelors       0.800000   

        marital-status          occupation    relationship    race      sex  \
0        Never-married        Adm-clerical   Not-in-family   White     Male   
1   Married-civ-spouse     Exec-managerial         Husband   White     Male   
2             Divorced   Handlers-cleaners   Not-in-family   White     Male   
3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   
4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   

   capital-gain  capital-loss  hours-per-week  native-country  
0      0.667492           0.0        0.397959   United-States  
1      0.000000           0.0        0.122449   United-States  
2      0.000000           0.0        0.397959   United-States  
3      0.000000           0.0        0.397959   United-States  
4      0.000000           0.0        0.397959            Cuba  
    \end{verbatim}

    
    \subsubsection{Implementation: Data
Preprocessing}\label{implementation-data-preprocessing}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} One\PYZhy{}hot encode the \PYZsq{}features\PYZus{}log\PYZus{}minmax\PYZus{}transform\PYZsq{} data using pandas.get\PYZus{}dummies()}
        \PY{n}{features\PYZus{}final} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{features\PYZus{}log\PYZus{}minmax\PYZus{}transform}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Encode the \PYZsq{}income\PYZus{}raw\PYZsq{} data to numerical values}
        \PY{n}{income} \PY{o}{=} \PY{p}{(}\PY{n}{income\PYZus{}raw} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZgt{}50K}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Print the number of features after one\PYZhy{}hot encoding}
        \PY{n}{encoded} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{features\PYZus{}final}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
        \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ total features after one\PYZhy{}hot encoding.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{encoded}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        
        \PY{c+c1}{\PYZsh{}print (encoded)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
103 total features after one-hot encoding.

    \end{Verbatim}

    \subsubsection{Shuffle and Split Data}\label{shuffle-and-split-data}

80\% of the data will be used for training and 20\% for testing.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Import train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        
        \PY{c+c1}{\PYZsh{} Split the \PYZsq{}features\PYZsq{} and \PYZsq{}income\PYZsq{} data into training and testing sets}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{features\PYZus{}final}\PY{p}{,} 
                                                            \PY{n}{income}\PY{p}{,} 
                                                            \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,} 
                                                            \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Show the results of the split}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training set has }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ samples.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Testing set has }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ samples.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Training set has 36177 samples.
Testing set has 9045 samples.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}chanh\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}cross\_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model\_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Evaluating Model
Performance}\label{evaluating-model-performance}

    \subsubsection{Metrics and the Naive
Predictor}\label{metrics-and-the-naive-predictor}

\emph{CharityML}, equipped with their research, knows individuals that
make more than \$50,000 are most likely to donate to their charity.
Because of this, \emph{CharityML} is particularly interested in
predicting who makes more than \$50,000 accurately. It would seem that
using \textbf{accuracy} as a metric for evaluating a particular model's
performace would be appropriate. Additionally, identifying someone that
\emph{does not} make more than \$50,000 as someone who does would be
detrimental to \emph{CharityML}, since they are looking to find
individuals willing to donate. Therefore, a model's ability to precisely
predict those that make more than \$50,000 is \emph{more important} than
the model's ability to \textbf{recall} those individuals. We can use
\textbf{F-beta score} as a metric that considers both precision and
recall:

\[ F_{\beta} = (1 + \beta^2) \cdot \frac{precision \cdot recall}{\left( \beta^2 \cdot precision \right) + recall} \]

In particular, when \(\beta = 0.5\), more emphasis is placed on
precision. This is called the \textbf{F\(_{0.5}\) score} (or F-score for
simplicity).

Looking at the distribution of classes (those who make at most \$50,000,
and those who make more), it's clear most individuals do not make more
than \$50,000. This can greatly affect \textbf{accuracy}, since we could
simply say \emph{"this person does not make more than \$50,000"} and
generally be right, without ever looking at the data! Making such a
statement would be called \textbf{naive}, since we have not considered
any information to substantiate the claim. It is always important to
consider the \emph{naive prediction} for the data, to help establish a
benchmark for whether a model is performing well. That been said, using
that prediction would be pointless: If we predicted all people made less
than \$50,000, \emph{CharityML} would identify no one as donors.

\paragraph{Note: accuracy, precision,
recall}\label{note-accuracy-precision-recall}

** Accuracy ** measures how often the classifier makes the correct
prediction. It's the ratio of the number of correct predictions to the
total number of predictions (the number of test data points).

** Precision ** tells us what proportion of messages we classified as
spam, actually were spam. It is a ratio of true positives(words
classified as spam, and which are actually spam) to all positives(all
words classified as spam, irrespective of whether that was the correct
classificatio), in other words it is the ratio of

\texttt{{[}True\ Positives/(True\ Positives\ +\ False\ Positives){]}}

** Recall(sensitivity)** tells us what proportion of messages that
actually were spam were classified by us as spam. It is a ratio of true
positives(words classified as spam, and which are actually spam) to all
the words that were actually spam, in other words it is the ratio of

\texttt{{[}True\ Positives/(True\ Positives\ +\ False\ Negatives){]}}

For classification problems that are skewed in their classification
distributions like in our case, for example if we had a 100 text
messages and only 2 were spam and the rest 98 weren't, accuracy by
itself is not a very good metric. We could classify 90 messages as not
spam(including the 2 that were spam but we classify them as not spam,
hence they would be false negatives) and 10 as spam(all 10 false
positives) and still get a reasonably good accuracy score. For such
cases, precision and recall come in very handy. These two metrics can be
combined to get the F1 score, which is weighted average(harmonic mean)
of the precision and recall scores. This score can range from 0 to 1,
with 1 being the best possible F1 score(we take the harmonic mean as we
are dealing with ratios).

    \subsubsection{Question 1 - Naive Predictor
Performace}\label{question-1---naive-predictor-performace}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{income\PYZus{}pred}\PY{o}{=}\PY{n}{income}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{TP}\PY{o}{=}\PY{n+nb}{sum}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{1} \PY{o+ow}{and} \PY{n}{y}\PY{o}{==}\PY{l+m+mi}{1} \PY{k}{else} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{income}\PY{p}{,}\PY{n}{income\PYZus{}pred}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}True Pos}
         \PY{n}{FP}\PY{o}{=}\PY{n+nb}{sum}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{0} \PY{o+ow}{and} \PY{n}{y}\PY{o}{==}\PY{l+m+mi}{1} \PY{k}{else} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{income}\PY{p}{,}\PY{n}{income\PYZus{}pred}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}False Pos}
         \PY{n}{FN}\PY{o}{=}\PY{n+nb}{sum}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{:} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{1} \PY{o+ow}{and} \PY{n}{y}\PY{o}{==}\PY{l+m+mi}{0} \PY{k}{else} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{income}\PY{p}{,}\PY{n}{income\PYZus{}pred}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}False Neg}
         
         \PY{c+c1}{\PYZsh{} accuracy = TP/(TP+FP)}
         \PY{n}{accuracy} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n}{TP}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{TP}\PY{o}{+}\PY{n}{FP}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The commented code below was used to confirm the precision calculation was correct}
         \PY{c+c1}{\PYZsh{}accuracy1 = accuracy\PYZus{}score(income,income\PYZus{}pred)}
         \PY{c+c1}{\PYZsh{}print \PYZsq{}accuracy comparison\PYZsq{},accuracy,accuracy1}
         
         \PY{c+c1}{\PYZsh{} recall = TP/(TP+FN)}
         \PY{n}{recall}\PY{o}{=}\PY{n+nb}{float}\PY{p}{(}\PY{n}{TP}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{TP}\PY{o}{+}\PY{n}{FN}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The commented code below was used to confirm the recall calculation was correct}
         \PY{c+c1}{\PYZsh{}recal1=recall\PYZus{}score(income,income\PYZus{}pred)}
         \PY{c+c1}{\PYZsh{}print \PYZsq{}recall comparison\PYZsq{},recal1,recall1}
         
         \PY{c+c1}{\PYZsh{} Calculate F\PYZhy{}score using the formula above for beta = 0.5}
         \PY{n}{beta}\PY{o}{=}\PY{l+m+mf}{0.5}
         \PY{n}{fscore} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{accuracy}\PY{o}{*}\PY{n}{recall}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{accuracy}\PY{o}{+}\PY{n}{recall}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Print the results }
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Naive Predictor: [Accuracy score: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{, F\PYZhy{}score: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{]}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{,} \PY{n}{fscore}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Naive Predictor: [Accuracy score: 0.2478, F-score: 0.2917]

    \end{Verbatim}

    \subsubsection{Supervised Learning
Models}\label{supervised-learning-models}

\textbf{The following are some of the supervised learning models that
are currently available in}
\href{http://scikit-learn.org/stable/supervised_learning.html}{\texttt{scikit-learn}}
\textbf{that you may choose from:} - Gaussian Naive Bayes (GaussianNB) -
Decision Trees - Ensemble Methods (Bagging, AdaBoost, Random Forest,
Gradient Boosting) - K-Nearest Neighbors (KNeighbors) - Stochastic
Gradient Descent Classifier (SGDC) - Support Vector Machines (SVM) -
Logistic Regression

    \subsubsection{Question 2 - Model
Application}\label{question-2---model-application}

List three of the supervised learning models above that are appropriate
for this problem that you will test on the census data. For each model
chosen

\begin{itemize}
\tightlist
\item
  Describe one real-world application in industry where the model can be
  applied.
\item
  What are the strengths of the model; when does it perform well?
\item
  What are the weaknesses of the model; when does it perform poorly?
\item
  What makes this model a good candidate for the problem, given what you
  know about the data?
\end{itemize}

    \textbf{Answer: }

\begin{itemize}
\tightlist
\item
  Three models were selected to test out predicting the data, including
  Gaussian Naive Bayes, Support Vector Machines, and Ensemble Method
  (AdaBoost).
\end{itemize}

\textbf{Ensemble Method:AdaBoost}

\begin{itemize}
\item
  AdaBoost is a powerful classifier which works well on both basic and
  more complex recognition problems. It can be used in various fields
  like biology, computer vision, and speech proecssing. A real-world
  application in industry using AdaBoost is Face Detection/Facial
  Recognition.
\item
  The strengths of AdaBoost include: 1) requires tuning of fewer
  parameters, 2) simple to implement, 3) implicit feature selection, 4)
  provably effective given weak learning assumption, 5) performs very
  well in practice, and 6) produces fairly good generalization. When
  there is low noise and few outliers, AdaBoost performs well.
\item
  The weaknesses of AdaBoost include: 1) suboptimal solution, 2)
  sensitive to noisy data and outliers, 3) weak classifiers too complex
  could lead to overfitting, 4) weak classifiers too weak can lead to
  low margins and overfitting. Therefore, when there are noises or
  outliers, or weak learners being too complex, or weak learners being
  too weak can lead to poor performance of AdaBoost.
\item
  AdaBoost uses a combination of multiple weak learners, which then
  turns into a stronger learner. Since the goal is to classify who would
  more likely be a donor and there are many features in this dataset,
  using AdaBoost can be a simple solution because of its implicit
  feature selection.
\end{itemize}

\textbf{Gaussian Naive Bayes}

\begin{itemize}
\item
  Naive Bayes is a collection of classification algorithms based on
  Bayes Theorem. The principle is that every feature being classified is
  independent of the value of any other feature. However, features
  aren't always independent which is often seen as a shortcoming of the
  Naive Bayes algorithm and the reason why it is called 'naive'.
  Gaussian Naive Bayes is specifically used when the features have
  continuous values. It is also assumed that all the features are
  following a gaussian distribution (i.e., normal distribution). Its
  real-world application examples include: classify emails as spam or
  ham, classify emails promise an attachment, and classify which web
  pages are student home pages. (source1: Aylien's blog,
  http://blog.aylien.com/naive-bayes-for-dummies-a-simple-explanation/),
  source2: dataaspirant.com,
  http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/)
\item
  Advantages of Gaussian Naive Bayes: 1) simple to understand and
  implement, 2) easy to train, 3) computationally fast, 4) works well
  with high dimensions. It performs well when all features are
  continuous.
\item
  Disadvantage of Gaussian Naive Bayes: relies on independence
  assumption. Gaussian Naive Bayes will perform badly if this
  independence assumption is not met.
\item
  All features of the data in this project are categorical. Therefore it
  is chosen to make predictions.
\end{itemize}

\textbf{Support Vector Machines}

\begin{itemize}
\item
  A support vector machine (SVM) is a supervised machine learning
  algorithm that can be employed for both classification and regression
  purposes. SVMs are more commonly used in classfication problems. SVMs
  are based on the idea of finding a hyperplane that best divides a
  dataset into two classes. Its real-world application examples include:
  face detection, text and hypertext categorization, classification of
  images, bioinformatics, sentiment analysis, handwriting recognition
  and generalized predictive control. (source1: KDnuggets,
  https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html,
  source2: data-flair,
  https://data-flair.training/blogs/applications-of-svm/)
\item
  Advantages of SVMs: 1) performs similarly to logistic regression when
  linear separation, 2) performs well with non-linear boundary depending
  on the kernel used, 3) handles high dimensional data well, 4) it
  produces high accuracy predictions, 5) works well on smaller cleaner
  datasets, 6) efficient due to the usage of a subset of training
  points. When a dataset is clean and relatively small, SVMs can perform
  well. (source1:
  https://github.com/ctufts/Cheat\_Sheets/wiki/Classification-Model-Pros-and-Cons,
  source2: KDnuggets,
  https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html)
\item
  Disadvantages: 1) is not suited to larger datasets as the training
  time with SVMs can be high, 2) less effective on noisier datasets with
  overlapping classes, and 3) susceptible to overfitting/training issues
  depending on kernel. So when a dataset has such aforementioned issues,
  SVMs would not perform well on it.
\item
  Although this dataset is large, SVM can still be considered as a good
  candidate for making predictions because the features in this dataset
  do not seem to overlap and could have a clear boundary to determine
  the income level.
\end{itemize}

    \subsubsection{Implementation - Creating a Training and Predicting
Pipeline}\label{implementation---creating-a-training-and-predicting-pipeline}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} Import two metrics from sklearn \PYZhy{} fbeta\PYZus{}score and accuracy\PYZus{}score}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{fbeta\PYZus{}score} 
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
         \PY{k}{def} \PY{n+nf}{train\PYZus{}predict}\PY{p}{(}\PY{n}{learner}\PY{p}{,} \PY{n}{sample\PYZus{}size}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{:} 
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    inputs:}
         \PY{l+s+sd}{       \PYZhy{} learner: the learning algorithm to be trained and predicted on}
         \PY{l+s+sd}{       \PYZhy{} sample\PYZus{}size: the size of samples (number) to be drawn from training set}
         \PY{l+s+sd}{       \PYZhy{} X\PYZus{}train: features training set}
         \PY{l+s+sd}{       \PYZhy{} y\PYZus{}train: income training set}
         \PY{l+s+sd}{       \PYZhy{} X\PYZus{}test: features testing set}
         \PY{l+s+sd}{       \PYZhy{} y\PYZus{}test: income testing set}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             
             \PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{} Fit the learner to the training data using slicing with \PYZsq{}sample\PYZus{}size\PYZsq{} using .fit(training\PYZus{}features[:], training\PYZus{}labels[:])}
             \PY{n}{start} \PY{o}{=} \PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Get start time}
             \PY{n}{learner}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{sample\PYZus{}size}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{n}{sample\PYZus{}size}\PY{p}{]}\PY{p}{)}
             \PY{n}{end} \PY{o}{=} \PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Get end time}
             
             \PY{c+c1}{\PYZsh{} Calculate the training time}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{end} \PY{o}{\PYZhy{}} \PY{n}{start}
                 
             \PY{c+c1}{\PYZsh{} Get the predictions on the test set(X\PYZus{}test),}
             \PY{c+c1}{\PYZsh{}       then get predictions on the first 300 training samples(X\PYZus{}train) using .predict()}
             \PY{n}{start} \PY{o}{=} \PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Get start time}
             \PY{n}{predictions\PYZus{}test} \PY{o}{=} \PY{n}{learner}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
             \PY{n}{predictions\PYZus{}train} \PY{o}{=} \PY{n}{learner}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{)}
             \PY{n}{end} \PY{o}{=} \PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} Get end time}
             
             \PY{c+c1}{\PYZsh{} Calculate the total prediction time}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{end} \PY{o}{\PYZhy{}} \PY{n}{start}
                     
             \PY{c+c1}{\PYZsh{} Compute accuracy on the first 300 training samples which is y\PYZus{}train[:300]}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc\PYZus{}train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} \PY{n}{predictions\PYZus{}train}\PY{p}{)}
                 
             \PY{c+c1}{\PYZsh{} Compute accuracy on test set using accuracy\PYZus{}score()}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc\PYZus{}test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions\PYZus{}test}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Compute F\PYZhy{}score on the the first 300 training samples using fbeta\PYZus{}score()}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{]}\PY{p}{,} \PY{n}{predictions\PYZus{}train}\PY{p}{,} \PY{n}{beta}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
                 
             \PY{c+c1}{\PYZsh{} Compute F\PYZhy{}score on the test set which is y\PYZus{}test}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions\PYZus{}test}\PY{p}{,} \PY{n}{beta}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
                
             \PY{c+c1}{\PYZsh{} Success}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ trained on }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ samples.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{learner}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,} \PY{n}{sample\PYZus{}size}\PY{p}{)}\PY{p}{)}
                 
             \PY{c+c1}{\PYZsh{} Return the results}
             \PY{k}{return} \PY{n}{results}
\end{Verbatim}


    \subsubsection{Implementation: Initial Model
Evaluation}\label{implementation-initial-model-evaluation}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Import the three supervised learning models from sklearn}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{GaussianNB}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{AdaBoostClassifier}
         
         \PY{c+c1}{\PYZsh{} Initialize the three models}
         \PY{n}{clf\PYZus{}A} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}
         \PY{n}{clf\PYZus{}B} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf\PYZus{}C} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Calculate the number of samples for 1\PYZpc{}, 10\PYZpc{}, and 100\PYZpc{} of the training data}
         \PY{n}{samples\PYZus{}1} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.01}\PY{p}{)}
         \PY{n}{samples\PYZus{}10} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.1}\PY{p}{)}
         \PY{n}{samples\PYZus{}100} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Collect results on the learners}
         \PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
         \PY{k}{for} \PY{n}{clf} \PY{o+ow}{in} \PY{p}{[}\PY{n}{clf\PYZus{}A}\PY{p}{,} \PY{n}{clf\PYZus{}B}\PY{p}{,} \PY{n}{clf\PYZus{}C}\PY{p}{]}\PY{p}{:}
             \PY{n}{clf\PYZus{}name} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}
             \PY{n}{results}\PY{p}{[}\PY{n}{clf\PYZus{}name}\PY{p}{]} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{samples} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{p}{[}\PY{n}{samples\PYZus{}1}\PY{p}{,} \PY{n}{samples\PYZus{}10}\PY{p}{,} \PY{n}{samples\PYZus{}100}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{results}\PY{p}{[}\PY{n}{clf\PYZus{}name}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PYZbs{}
                 \PY{n}{train\PYZus{}predict}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Run metrics visualization for the three supervised learning models chosen}
         \PY{n}{vs}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{results}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,} \PY{n}{fscore}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
GaussianNB trained on 361 samples.
GaussianNB trained on 3617 samples.
GaussianNB trained on 36177 samples.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}chanh\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}metrics\textbackslash{}classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn\_for)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
SVC trained on 361 samples.
SVC trained on 3617 samples.
SVC trained on 36177 samples.
AdaBoostClassifier trained on 361 samples.
AdaBoostClassifier trained on 3617 samples.
AdaBoostClassifier trained on 36177 samples.

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Improving Results}\label{improving-results}

    \subsubsection{Question 3 - Choosing the Best
Model}\label{question-3---choosing-the-best-model}

    \textbf{Answer: } From the result above, AdaBoost seems to be the most
appropriate model for the task of identifying individuals that make more
than \$50K. Compared to AdaBoost and Support Vector Machine, Gaussian
Naive Bayes did not do a good job at achieving a high accuracy during
training and testing. As well, Gaussian Naive Bayes only achieved a
f-score lower than 0.5 during both training and testing. Therefore,
Gaussian Naive Bayes was not chosen to complete this task of finding a
donor. Although both AdaBoost and Support Vector Machine have similar
accuracty score, AdaBoost has slightly higher f-score. F-score is a
measure that combines precision and recall, which is the harmonic mean
of precision and recall. Depending on the task, f-score could be used as
a reference for selecting an appropriate model. In the case of finding
donors, precision is more important than recall. That suggests that a
model with a f-score closer to 1 is a better fit for the task. This
means that AdaBoost tends to produce a more precise prediction. In
addition, given that not all features of this dataset are independent,
Gaussian Naive Bayes may not be a suitable model.

 With all the attributes considered, I feel that AdaBoost would be the
most appropriate model for completing this task.

    \subsubsection{Question 4 - Describing the Model in Layman's
Terms}\label{question-4---describing-the-model-in-laymans-terms}

\begin{itemize}
\tightlist
\item
  In one to two paragraphs, explain to \emph{CharityML}, in layman's
  terms, how the final model chosen is supposed to work. Be sure that
  you are describing the major qualities of the model, such as how the
  model is trained and how the model makes a prediction. Avoid using
  advanced mathematical jargon, such as describing equations.
\end{itemize}

    \textbf{Answer: } Adaboost is a boosting type ensemble learner. This
method works by combining multiple individual "weak" learning hypotheses
to create one strong model. Each weak hypothesis used is better at
classifying the data than random chance. However, it's the combination
of all of these independent weak learning hypotheses what makes the
model more capable of predicting accurately on unseen data than each of
the individual hypothesis would.

A weak learner is any machine learning algorithm that gives better
accuracy than simply guessing. For instance, if you are trying to
classify animals at a zoo, you might have an algorithm that can
correctly identify zebras most of the time, but it simply guesses for
any other animal. That algorithm would be a weak learner because it is
better than guessing.

If you had an algorithm that identified every animal as a zebra, then
that probably is not better than guessing and so it would not be a weak
learner.

For boosting problems, the best kinds of weak learners are ones that are
very accurate, even if it is only over a limited scope of the problem.
For instance the algorithm that correctly identifies zebras would be
good. It allows you to confidently identify as least most of the zebras,
allowing other weak learners to focus on the remaining animals. Boosting
algorithms typically work by solving subsections of the problem, by
peeling them away so future boosting iterations can solve the remaining
sections.

Imagine you are hiring people to build your house, and you have 10
different big jobs that need to be done. A great way of doing it would
be to get someone who is really good at foundations to build the
foundation. Then hire a very good carpenter to focus on the framing.
Then hire a great roofer and plumber to focus stage, a small subsection
of the project is getting completely solved.

The takeaway is that weak learners are best combined in a way that
allows each one to solve a limited section of the problem. Any machine
learning routine can be used as a weak learner. Neural nets, support
vector machines or any other would work, but the most commonly used weak
learner is the decision tree.

So how exactly does boosting work in this method? The way boosting works
to build a stronger classifier is through building a first model with
the training data. Then build a second model to correct the error
generates by the first model. This process repeats which results in more
models being built until it predicts the training data perfectly or
until it reaches the maximum number of models added. For example, you
wish to use the US Census data to predict people's income. Then first
you would need to separate the data into two parts, including 80\% of
which being the training set and 20\% being the testing data set. Why do
you need to do that? It is because that you would want to first train
the model then being able to test how well your model performs in
prediction. Next, you will show your first model the training data. You
will get an error rate after it finishes predicting the training data.
Then, you will build a second model which can then correct the errors
made by the first model and trying to increase the number of results
being correctly predicted. This process repeats until you get to a
perfect prediction score or the most number of models you can add. This
is how you boost the model.

    \subsubsection{Implementation: Model
Tuning}\label{implementation-model-tuning}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Import \PYZsq{}GridSearchCV\PYZsq{}, \PYZsq{}make\PYZus{}scorer\PYZsq{}, and any other necessary libraries}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{fbeta\PYZus{}score}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{AdaBoostClassifier}
         \PY{k+kn}{import} \PY{n+nn}{warnings}
         \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{always}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Initialize the classifier}
         \PY{n}{clf} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create the parameters list you wish to tune, using a dictionary if needed.}
         \PY{c+c1}{\PYZsh{} parameters = \PYZob{}\PYZsq{}parameter\PYZus{}1\PYZsq{}: [value1, value2], \PYZsq{}parameter\PYZus{}2\PYZsq{}: [value1, value2]\PYZcb{}}
         \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mi}{80}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{,} \PY{l+m+mf}{2.5}\PY{p}{]}\PY{p}{\PYZcb{}}
         
         \PY{c+c1}{\PYZsh{} Make an fbeta\PYZus{}score scoring object using make\PYZus{}scorer()}
         \PY{n}{scorer} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{fbeta\PYZus{}score}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Perform grid search on the classifier using \PYZsq{}scorer\PYZsq{} as the scoring method using GridSearchCV()}
         \PY{n}{grid\PYZus{}obj} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{n}{parameters}\PY{p}{,} \PY{n}{scoring} \PY{o}{=} \PY{n}{scorer}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Fit the grid search object to the training data and find the optimal parameters using fit()}
         \PY{n}{grid\PYZus{}fit} \PY{o}{=} \PY{n}{grid\PYZus{}obj}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get the estimator}
         \PY{n}{best\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
         
         \PY{c+c1}{\PYZsh{} Make predictions using the unoptimized and model}
         \PY{n}{predictions} \PY{o}{=} \PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{best\PYZus{}predictions} \PY{o}{=} \PY{n}{best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Report the before\PYZhy{}and\PYZhy{}afterscores}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Unoptimized model}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy score on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F\PYZhy{}score on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Optimized Model}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final accuracy score on the testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}predictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final F\PYZhy{}score on the testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}predictions}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Unoptimized model
------
Accuracy score on testing data: 0.8576
F-score on testing data: 0.7246

Optimized Model
------
Final accuracy score on the testing data: 0.8677
Final F-score on the testing data: 0.7452

    \end{Verbatim}

    \subsubsection{Question 5 - Final Model
Evaluation}\label{question-5---final-model-evaluation}

\begin{itemize}
\tightlist
\item
  What is your optimized model's accuracy and F-score on the testing
  data?
\item
  Are these scores better or worse than the unoptimized model?
\item
  How do the results from your optimized model compare to the naive
  predictor benchmarks you found earlier in \textbf{Question 1}?\_
\end{itemize}

    \paragraph{Results:}\label{results}

\begin{longtable}[]{@{}ccc@{}}
\toprule
Metric & Unoptimized Model & Optimized Model\tabularnewline
\midrule
\endhead
Accuracy Score & 0.8576 & 0.8677\tabularnewline
F-score & 0.7246 & 0.7452\tabularnewline
\bottomrule
\end{longtable}

    \textbf{Answer: } - The optimized model's accuracy score is 0.8677 and
the F-score is 0.7452. - These scores are better than the unoptimized
model. - When compared to the naive predictor benchmarks found in
Question 1, these results of the optimized model are significantly
better and higher.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Feature Importance}\label{feature-importance}

    \subsubsection{Question 6 - Feature Relevance
Observation}\label{question-6---feature-relevance-observation}

When \textbf{Exploring the Data}, it was shown there are thirteen
available features for each individual on record in the census data. Of
these thirteen records, which five features do you believe to be most
important for prediction, and in what order would you rank them and why?

    \textbf{Answer:} 1. Capital Gain: If one has more capital gain, this
person will be more likely to donate.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Occupation: The income of a person directly affect whether or not a
  person would donate. One's occupation can give a pretty clear idea as
  to how much salary this person makes. Therefore I rank 'Occupation' as
  the number one feature.
\item
  Capital loss: If one has more capital loss, I don't think this person
  will be likely to donate.
\item
  Relationship Status: If one is married and has children, maybe he or
  she would not be as likely to donate due to the future investment and
  spending on their children.
\item
  Education: I feel that one with higher education may be more likely to
  have a higher paying job, which could suggest that the person would be
  more likely to donate.
\end{enumerate}

    \subsubsection{Implementation - Extracting Feature
Importance}\label{implementation---extracting-feature-importance}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} Import a supervised learning model that has \PYZsq{}feature\PYZus{}importances\PYZus{}\PYZsq{}}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{ExtraTreesClassifier}
         
         \PY{c+c1}{\PYZsh{} Train the supervised model on the training set using .fit(X\PYZus{}train, y\PYZus{}train)}
         \PY{n}{model} \PY{o}{=} \PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} Extract the feature importances using .feature\PYZus{}importances\PYZus{} }
         \PY{n}{importances} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}
         
         \PY{c+c1}{\PYZsh{} Plot}
         \PY{n}{vs}\PY{o}{.}\PY{n}{feature\PYZus{}plot}\PY{p}{(}\PY{n}{importances}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Question 7 - Extracting Feature
Importance}\label{question-7---extracting-feature-importance}

Observe the visualization created above which displays the five most
relevant features for predicting if an individual makes at most or above
\$50,000.\\
* How do these five features compare to the five features you discussed
in \textbf{Question 6}? * If you were close to the same answer, how does
this visualization confirm your thoughts? * If you were not close, why
do you think these features are more relevant?

    \textbf{Answer:} - My opions on the top five important features are not
completely align with the assessement results above. I only got capital
gain, capital loss and education right.

\begin{itemize}
\item
  From the bar chart above, capital gain has the heaviest feature
  weight, followed by capital loss, age, hours-per-week, and education.
  In my answers to Question 6, I ranked capital gain as number one,
  capital loss as number three, and education as number five. The
  ranking was pretty closed to the assessment result.
\item
  I did not think about the number of hours one works per week would
  have an impact on the decision to donation. This took me by suprise.
  Because I think occupation and work-class may be better features to
  predict the income of a person, but it does make senese at a certain
  level that if paid at the same rate, one works more hours per week
  will have more income. As far as age, I did not think it would play a
  role. But I can also understand that younger people may not have as
  high income as older people given their years of experience.
\end{itemize}

    \subsubsection{Feature Selection}\label{feature-selection}

How does a model perform if we only use a subset of all the available
features in the data? With less features required to train, the
expectation is that training and prediction time is much lower --- at
the cost of performance metrics. From the visualization above, we see
that the top five most important features contribute more than half of
the importance of \textbf{all} features present in the data. This hints
that we can attempt to \emph{reduce the feature space} and simplify the
information required for the model to learn. The code cell below will
use the same optimized model you found earlier, and train it on the same
training set \emph{with only the top five important features}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} Import functionality for cloning a model}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{base} \PY{k}{import} \PY{n}{clone}
         
         \PY{c+c1}{\PYZsh{} Reduce the feature space}
         \PY{n}{X\PYZus{}train\PYZus{}reduced} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{importances}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{]}
         \PY{n}{X\PYZus{}test\PYZus{}reduced} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{importances}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Train on the \PYZdq{}best\PYZdq{} model found from grid search earlier}
         \PY{n}{clf} \PY{o}{=} \PY{p}{(}\PY{n}{clone}\PY{p}{(}\PY{n}{best\PYZus{}clf}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}reduced}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Make new predictions}
         \PY{n}{reduced\PYZus{}predictions} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}reduced}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Report scores from the final model using both versions of data}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final Model trained on full data}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}predictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F\PYZhy{}score on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}predictions}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Final Model trained on reduced data}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{reduced\PYZus{}predictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F\PYZhy{}score on testing data: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{fbeta\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{reduced\PYZus{}predictions}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Final Model trained on full data
------
Accuracy on testing data: 0.8677
F-score on testing data: 0.7452

Final Model trained on reduced data
------
Accuracy on testing data: 0.8421
F-score on testing data: 0.7003

    \end{Verbatim}

    \subsubsection{Question 8 - Effects of Feature
Selection}\label{question-8---effects-of-feature-selection}

\begin{itemize}
\tightlist
\item
  How does the final model's F-score and accuracy score on the reduced
  data using only five features compare to those same scores when all
  features are used?
\item
  If training time was a factor, would you consider using the reduced
  data as your training set?
\end{itemize}

    \textbf{Answer:} - The final model's F-score and accuracy score are
lower than those of the previous model. This makes sense because the
five selected features did not provide the full picture of the dataset.
Because other than these five features, there are other factors that
could affect the likelihood of someone's decision on donation.

\begin{itemize}
\tightlist
\item
  If training time was a factor, I would consider using the reduced data
  as my training set because although the F-score and accuracy score
  were lower, they were not that significantly low. The small increase
  in classification error could be an appropriate tradeoff in that case.
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
